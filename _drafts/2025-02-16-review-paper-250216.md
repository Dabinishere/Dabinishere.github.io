---
revision: 0
title: "MUG study 250217"
category: programming
tags:
  - review
created_at: 2025-02-16 16:55:21 +09:00
last_modified_at: 2025-02-16 16:55:21 +09:00
excerpt: "논문 리뷰 - 0216"
---

# 레시피

FoodLLM의 환각 문제를 지적하면서, 쿼리 시퀀스와 유사성이 높은 상위 K 문서를 검색해서 꺼내옴. 생성 정보를 평가해서 필터링 하는 방식으로 self-consistency ensemble voting처럼 전통적인 ML방법의 도입도 생각해볼만함. RAG 아이디어를 가져와서 정보를 삽입할 수 있다면 좋을 것 같음





# Prompt 논문

결합하기 쉬운 방법들이 많음, 일반화 쪽에 좀 더 초점이 맞추어져 있는듯 함 (Lora 등)


# TULIP
Long-CLIP은 토큰과의 관계를 알기 어려움.

# 히정님 KD 논문
rep gap을 줄이는 것에 집중한 이전 연구들 => 그래프 구성은 동질성(homophily)에 기반을 두어서 유사한 클래스 내 학습만 집중함. Point distiller라는 방식을 통해서 샘플링하면서 계산량과 견고성을 함께 잡음.
유클리디언 거리를 평균 후 정규화, 벡터를 내적해서 각도 기반 관계 모델링
Huber loss (이상치 처리에 좋은 로스)





# 0227 KD 스터디

## 동훈님 논문 inter-channel correlation
1.  코릴레이션 채널간 판단 + grid 넣어서 시행하자.
2. 교사가 좋으면 학생도 좋아질까? 갭이 너무 크면 오히려 따라잡지 못한다.

## 채영님 논문 3D Pretraining
1. 2D 3D 도메인 차이가 있어서 feature distillation은 한계가 있음 => 추가 라벨없이 2D 데이터로 향상 어떻게 시키지? => concept level에서 맞추어보자
2. 컨셉 쿼리를 만들자 (CA) 그리고 2D 이미지피처에서 caption mapping한 결과물 토큰과 distill loss... 왜 되는거지?
3. 수렴이 빨리된다는 건 너무 쉬운 task이고 최적성능이 아닐수 있다?



# 0306 스터디

## 정훈님 논문
1. 직교 프로젝션 층: T-S 차원 달라서 projection 해야하지만 내적 유지를 위해 직교 프로젝션함함
2. whitening: 데이터 분포를 균일하게 만드는 과정으로 상관관계를 제거해서 각 특성이 동일한 분산을 갖도록 함 (generative task - 특히 gan에서 중요) (teacher에서만 사용용)
3. 교사-학생 간 용량 차이가 커도 성능 제한 없음
4. 피처를 그대로 따라하고 header만 재사용하면 된다던 논문과 비슷한 맥락
5. linear layer나 conv 통과 등으로 차원 맞추는 게 예전상황

## 희정님 논문 -DGD (ECCV2024)
1. 2D semantic feature를 3DGS 장면에 증류하는 것
2. 다시점 비디오를 input => SfM으로 포인트 클라우드 생성 => 3D 가우시안
3. semantic feature 삽임 (다른 논문) + Deformable (다른 논문)
4. 실험시 3K iter는 정적 가우시안으로 warmup


# 0310 MUG

## 허찬님 논문 - DREAM
1. "This is a hard problem"이라는 프롬프트를 넣어주면 잘 한다
2. 자유롭게 LLM이 생성하라고 했을 때 효과가 있다?
3. TPAMI로 올릴 논문을 생각할 때 참고할 지점이 있을 것 같음

> RAG로 생성 텍스트 샘플링해서 쿼리 생성 모델을 adaptation할 수 있을지? (쿼리 스타일 adaptation) + 멀티모달RAG로 비디오 쿼리도 RAG로 넣어줄 수 있음, 추론에서만 사용? 학습에서 사용?
> Long-CLIP으로 생성 많이한 문장 모두 넣을 수 있도록 해보자?


## 동훈님 논문 - UCDR-Adapter (WACV 2025)
1. UCDR task: 도메인(데이터 특성) + 클래스(해당 클래스가 들어있는 것) 를 모두 unseen한 상황에서 학습
2. 일반화 해결을 위해 프롬프트를 도입함
3. Frozen해두고 prompt를 넣어서 훈련시킬 때, 기존 도메인 지식을 유지할 수 있다는 장점?
4. prompt는 domain adaptation에서 해결책?

## 정훈님 논문 - effective post-training embedding compression (ICLR 2025 spotlight)
1. optimal temperature parameter 구하는 법?
2. 임베딩 압축으로 멀티모달?

