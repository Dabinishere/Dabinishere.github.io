---
revision: 0
title: "MUG study 250217"
category: programming
tags:
  - review
created_at: 2025-02-16 16:55:21 +09:00
last_modified_at: 2025-02-16 16:55:21 +09:00
excerpt: "논문 리뷰 - 0216"
---

# 레시피

FoodLLM의 환각 문제를 지적하면서, 쿼리 시퀀스와 유사성이 높은 상위 K 문서를 검색해서 꺼내옴. 생성 정보를 평가해서 필터링 하는 방식으로 self-consistency ensemble voting처럼 전통적인 ML방법의 도입도 생각해볼만함. RAG 아이디어를 가져와서 정보를 삽입할 수 있다면 좋을 것 같음





# Prompt 논문

결합하기 쉬운 방법들이 많음, 일반화 쪽에 좀 더 초점이 맞추어져 있는듯 함 (Lora 등)


# TULIP
Long-CLIP은 토큰과의 관계를 알기 어려움.

# 히정님 KD 논문
rep gap을 줄이는 것에 집중한 이전 연구들 => 그래프 구성은 동질성(homophily)에 기반을 두어서 유사한 클래스 내 학습만 집중함. Point distiller라는 방식을 통해서 샘플링하면서 계산량과 견고성을 함께 잡음.
유클리디언 거리를 평균 후 정규화, 벡터를 내적해서 각도 기반 관계 모델링
Huber loss (이상치 처리에 좋은 로스)





# 0227 KD 스터디

## 동훈님 논문 inter-channel correlation
1.  코릴레이션 채널간 판단 + grid 넣어서 시행하자.
2. 교사가 좋으면 학생도 좋아질까? 갭이 너무 크면 오히려 따라잡지 못한다.

## 채영님 논문 3D Pretraining
1. 2D 3D 도메인 차이가 있어서 feature distillation은 한계가 있음 => 추가 라벨없이 2D 데이터로 향상 어떻게 시키지? => concept level에서 맞추어보자
2. 컨셉 쿼리를 만들자 (CA) 그리고 2D 이미지피처에서 caption mapping한 결과물 토큰과 distill loss... 왜 되는거지?
3. 수렴이 빨리된다는 건 너무 쉬운 task이고 최적성능이 아닐수 있다?
