---
revision: 0
title: "Study Topic 250314"
category: programming
tags:
  - toefl
created_at: {{ page | created_at }}
last_modified_at: {{ page | last_modified_at }}
excerpt: "Study Topic"
---


# 자율주행을 위한 Depth Estimation 최신 연구 동향 (2020~2025)

## 1. LiDAR 기반 Depth Estimation 연구 동향  
**LiDAR-카메라 융합**을 통한 Depth Estimation
- LiDAR 센서
  
  - (장점) 정확한 거리 정보를 제공
  - (단점) 포인트가 **희소(sparse)**하여 바로 고해상도 뎁스 맵을 얻기 어려움
  - (해결) RGB 카메라 영상과 LiDAR 데이터를 결합하는 **Depth Completion** 기법
  ([Depth Completion | Papers With Code](https://paperswithcode.com/task/depth-completion#:~:text=The%20,Depth%20Completion%20from%20Visual%20Inertial))
    - (예시) **KITTI Depth Completion 벤치마크**에서 2020년대 초반 등장한 **NLSPN (Non-Local Spatial Propagation Network)** 은 LiDAR의 희소 깊이값을 주변 픽셀의 비국소적 관계로 전파 ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=67%20NLSPN%20code%201,European%20Conference%20on))
    - **RigNet (ECCV 2022)** 은 반복적 이미지-깊이 피처 결합으로 정밀도를 향상 ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=39%20RigNet%202,ECCV%202022))
    - **LRRU (ICCV 2023)** 는 장단기 순환 업데이트로 실시간 처리에 근접하는 개선 ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=17%20HFFNet%20%201,ICCV%29%202023)). - 최신 **Tri-Perspective (CVPR 2024)**와 같은 기하학적 분해기법, **Bilateral Propagation Network (CVPR 2024)** 등 **Transformer**나 **양방향 필터링** 개념을 도입한 네트워크들이 등장 ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=8%20BP,CVPR%202024)) ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=2,CVPR%20%28oral%29%202024)).

  - (해결?) LiDAR의 **점 밀도 부족 문제**를 해결하기 위한 시도
    - 초저해상도 **SPAD LiDAR** 등의 자료를 업샘플링하는 연구에서는, 8×8 픽셀의 깊이 지도를 RGB 이미지로 보완하여 256×256의 고해상도로 복원하는 경량 신경망이 제안 ([A 256 &times; 256 LiDAR Imaging System Based on a 200 mW SPAD-Based SoC with Microlens Array and Lightweight RGB-Guided Depth Completion Neural Network](https://www.mdpi.com/1424-8220/23/15/6927#:~:text=neural%20network%20to%20upsample%20the,PDP%29%20of%20the)). 
  - (해결) **멀티뷰 LiDAR 융합**도 중요한데, 여러 프레임의 LiDAR 포인트를 누적하거나 멀티 카메라와 결합해 장거리 뎁스를 추정하는 기법이 개발
    - (예시) **SuperFusion** 네트워크는 **LiDAR 깊이로 카메라 기반 깊이 추정을 보정**하고, 반대로 **이미지 특징으로 LiDAR의 장거리 예측을 보완**하는 멀티레벨 융합을 통해 90m 이상의 장거리 HD 맵에서도 높은 정확도를 달성 ([SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map Generation](https://arxiv.org/html/2211.15656v3#:~:text=driving,com%2Fhaomo)).
-  LiDAR의 정확도와 카메라의 밀도를 상호 보완하는 융합 기법

## 2. 기타 Depth Estimation 기법 (Monocular, Stereo, Multi-view, 특수 센서)  
- **단안 (Monocular) 깊이 추정**
  - 카메라 한 대로 주변 환경의 깊이를 추정하는 방법
  - 2020년 이후 **자기지도(Self-Supervised)** 학습 기법의 발전으로 LiDAR나 스테레오 GT 없이도 모노큘러 깊이 예측 모델을 학습하는 사례
    - **PackNet-SfM (CVPR 2020)** 은 시퀀스 영상의 기하학적 제약을 이용해 학습
    - **ManyDepth (CVPR 2021)**는 여러 연속 프레임을 활용해 정지 물체와 움직이는 물체를 구분하여 정확도를 개선
    - **Transformer** 기반 접근도 등장하여, Vision Transformer를 활용한 **DPT (Dense Prediction Transformer, 2021)**나 **AdaBins (CVPR 2021)** 등이 **글로벌 문맥을 포착**함으로써 모노큘러 깊이 정확도를 크게 높임 ([AdaBins: Depth Estimation using Adaptive Bins | Papers With Code](https://paperswithcode.com/paper/adabins-depth-estimation-using-adaptive-bins#:~:text=We%20address%20the%20problem%20of,depth%20datasets%20across%20all%20metrics))
    - AdaBins의 경우 **심도 범위를 적응적으로 bin으로 나누는 모듈**을 도입하여 각 이미지의 깊이 분포에 맞게 예측함으로써 기존 CNN 기반 기법 대비 정확한 거리 추정을 달성 ([AdaBins: Depth Estimation using Adaptive Bins | Papers With Code](https://paperswithcode.com/paper/adabins-depth-estimation-using-adaptive-bins#:~:text=We%20address%20the%20problem%20of,depth%20datasets%20across%20all%20metrics))
    - 모노큘러 깊이 추정의 절대 상대 오류(Abs Rel)가 KITTI 데이터셋 기준 2019년 ~0.10 수준에서 최근 ~0.05 수준까지 절반 감소하는 등 큰 향상을 보이고 있음
    - **스케일 모호성(scale ambiguity)** 문제는 여전히 남아있어, 최근에는 **측심센서**(예: 바닥면의 알려진 크기)나 **딥러닝 기반 스케일 보정**을 추가하는 연구가 진행

- **스테레오(depth from stereo)** 기법도 동시에 발전
  - 딥러닝 이전에 널리 쓰이던 정합(cost aggregation) 기반 방법들을 대체하여, 3D cost volume을 학습하는 **PSMNet(2018)** 이후 **GA-Net(2019)**, **LEAStereo(2020)** 등 CNN 기반 기법
  - 2021년 이후로는 **Transformer 기반 스테레오**가 등장하여, 예컨대 **STTR (Stereo Transformer, ICCV 2021)**은 전통적 **고정 disparity 범위** 제한을 없애고 시퀀스-투-시퀀스 **어텐션 매칭**으로 **비약적으로 향상된 정합 품질** ([Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective With Transformers](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Revisiting_Stereo_Depth_Estimation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_ICCV_2021_paper.pdf#:~:text=dense%20pixel%20matching%20using%20position,tuning.%201.%20Introduction))
  - STTR은 **픽셀 단위로 직접 매칭**하여 **큰 범위의 깊이**도 다룰 수 있고, **가려짐(occlusion) 영역 식별과 매칭의 유일성 제약**까지 모델에 포함하여 이전 방식의 한계를 극복 ([Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective With Transformers](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Revisiting_Stereo_Depth_Estimation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_ICCV_2021_paper.pdf#:~:text=dense%20pixel%20matching%20using%20position,tuning.%201.%20Introduction))
  - 스테레오 매칭 오류(D1 오류)가 기존 대비 크게 감소했고, 실내/실외 다양한 도메인에 **사전학습 없이 일반화**되는 성능을 입증
  - **RAFT-Stereo (NeurIPS 2021)**처럼 **GRU 기반 반복정합** 기법이나 **CREStereo (CVPR 2022)** 등의 **혼합어텐션** 기법이 등장하여 거의 실시간에 가까운 속도로 정밀한 스테레오 깊이 지도를 얻는 방향

- **다중 뷰(Multi-view) 깊이 추정**
  - 여러 시점의 카메라 영상으로 3차원 구조를 복원하는 방법
  - 정지된 환경에서는 **MVSNet(2018)** 이후 **CasMVSNet(2020)**, **Vis-MVSNet(2021)** 등이 딥러닝 기반 다중뷰 스테레오를 발전시켜왔으며, 이는 자율주행차의 **동시지도작성(SLAM)**이나 **HD맵 생성**에도 활용
  - 동적인 자율주행 시나리오에서는 연속된 단안 영상들을 활용한 심층 네트워크가 구조정보를 누적하여 깊이를 추정
  - 사실상 **자기차량의 이동을 활용한 스테레오**로 볼 수 있으며, **듀얼 픽셀 카메라**처럼 한 카메라 내의 두 광축 정보를 사용하는 특수기법도 연구되고 
  - 다중뷰 방법은 **명시적인 3D 공간 비용(volume)**을 형성하여 최적화하는 경향이 있는데, 이는 매우 높은 정확도의 3D 재구성을 가능하게 하지만 계산량이 많아 **실시간성**이 이슈
  - 최근 연구들은 **NeRF(Neural Radiance Fields)** 등을 활용해 연속적인 장면 표현으로부터 깊이를 추정하거나, 필요 시 **부분적인 MVS**를 수행하는 등 효율화에 초점

- **특수 센서**
  - **이벤트 카메라**의 경우, μs 단위 고속으로 밝기 변화 이벤트 스트림을 제공하여 **조도 변화나 고속 운동 환경에서 효과적인 깊이 추정**을 가능([Self-supervised Event-based Monocular Depth Estimation using Cross-modal Consistency](https://arxiv.org/html/2401.07218v1#:~:text=An%20event%20camera%20is%20a,depth%20estimation%20framework%20named%20EMoDepth))
  - **EMoDepth (2024)**는 이벤트 카메라와 일반 카메라 프레임을 결합한 자기지도 학습을 도입하여 **레이블 없이도** 이벤트만으로 깊이를 추정 ([Self-supervised Event-based Monocular Depth Estimation using Cross-modal Consistency](https://arxiv.org/html/2401.07218v1#:~:text=supervised%20event,based%20methods))
  - **이벤트-프레임 간의 대응 일관성**을 손실로 활용하여 학습하며, **추론 시에는 이벤트 데이터만으로** 뎁스 맵을 얻으면서도 기존 프레임 기반 지도 학습 모델에 필적하거나 앞서는 정확도 ([Self-supervised Event-based Monocular Depth Estimation using Cross-modal Consistency](https://arxiv.org/html/2401.07218v1#:~:text=supervised%20event,based%20methods))
  - **스테레오 이벤트 카메라**를 활용한 연구에서는, 이벤트 데이터와 동시 촬영된 **intensity 프레임**을 함께 입력하여 각각의 약점을 보완한 사례
  - **Event-Intensity Stereo (ICCV 2021)** 연구는 **이벤트 스트림과 영상 프레임을 결합하여** 기존 이벤트만 혹은 영상만 이용할 때보다 **미세하고 정확한 깊이맵**을 얻음 ([Event-Intensity Stereo: Estimating Depth by the Best of Both Worlds](https://openaccess.thecvf.com/content/ICCV2021/papers/Mostafavi_Event-Intensity_Stereo_Estimating_Depth_by_the_Best_of_Both_Worlds_ICCV_2021_paper.pdf#:~:text=Figure%201,only%20stereo%20%28d%29%20pairs.%20Using)). 
  - 이벤트는 고속 움직임의 경계 디테일을 제공하고, 영상은 절대 밝기 정보를 제공하므로 두 데이터를 융합하면 **양측의 강점을 결합**할 수 있음을 보임  ([Event-Intensity Stereo: Estimating Depth by the Best of Both Worlds](https://openaccess.thecvf.com/content/ICCV2021/papers/Mostafavi_Event-Intensity_Stereo_Estimating_Depth_by_the_Best_of_Both_Worlds_ICCV_2021_paper.pdf#:~:text=event,imaging%20set%02tings%2C%20blurred%20or%20low))
  - 낮은 조도나 고속 주행 환경에서 유용하며, **DSEC** 등 자율주행용 이벤트-카메라 데이터셋을 통해 검증

- **적외선(IR) 카메라**나 **열화상 카메라**
  - 특히 야간이나 악천후 환경에서 **열상 카메라**는 물체 식별에, **근적외선 카메라**는 조도에 강인한 영상을 제공
  - 2025년 최신 연구에서는 **멀티-스펙트럼(depth from multi-spectral images)** 접근으로, **RGB+NIR+Thermal** 영상을 동시에 처리하여 어떤 환경에서도 안정적인 깊이 추정을 하는 시도
  - **Align-and-Fuse (2025)** 전략은 각 파장대 이미지 간 특징 임베딩을 **기하학적 정렬**로 맞추고, 이후 **신뢰도에 따라 가중치 합성**하는 모듈을 통해 **스펙트럼 간 공통된 깊이 표현**을 학습 ([Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning](https://arxiv.org/html/2503.00793v1#:~:text=Deploying%20depth%20estimation%20networks%20in,fuse%20strategy%2C%20for%20the%20depth))
  - 단일 네트워크가 **스펙트럼 불변성**을 가지면서도, 여러 센서를 함께 쓸 때는 **성능이 향상**되는 유연한 구조를 달성 ([Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning](https://arxiv.org/html/2503.00793v1#:~:text=Deploying%20depth%20estimation%20networks%20in,fuse%20strategy%2C%20for%20the%20depth))
  - 멀티-스펙트럼 접근은 안개, 어두움 등의 극한 상황에서 특정 센서에 의존하지 않고 안정적인 깊이 추정을 제공할 것으로 기대
- **Radar**(레이더) 센서
  - 저해상도이지만 긴 탐지거리를 활용하는 보조 깊이 정보원
  - mmWave 레이더는 환경 메쉬 정보를 희소하게 제공하는데, 2024년 **RadarCam-Depth** 연구에서는 **레이더 포인트를 모노큘러 네트워크에 결합**하여 **절대적인 거리 스케일**을 보정하는 방법을 제안 ([RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale](https://arxiv.org/html/2401.04325v2#:~:text=We%20present%20a%20novel%20approach,learning%20the%20association%20between%20Radar))
  - 영상 기반 모노큘러 추정은 상대적 깊이 형태이지만, 여기에 레이더의 (노이즈 많지만) **드문 점 거리**를 글로벌 스케일 정규화로 활용하면, **전역적인 정확도와 경계 세밀함**이 개선 ([RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale](https://arxiv.org/html/2401.04325v2#:~:text=We%20present%20a%20novel%20approach,learning%20the%20association%20between%20Radar))
  - Radar, 이벤트, 열화상 등 특수 센서를 활용한 깊이 추정은 **자율주행 센서 퓨전**의 중요한 흐름으로, 악조건에서도 견고한 인식 성능을 확보하는 방향으로 발전 ([Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning](https://arxiv.org/html/2503.00793v1#:~:text=Deploying%20depth%20estimation%20networks%20in,fuse%20strategy%2C%20for%20the%20depth)).

## 3. 기술적 접근법 분석 (네트워크 구조 및 학습 기법)  
- **CNN 기반 U-Net 아키텍처**는 여전히 기본 골격으로 많이 쓰이나, 여기에 **Transformer**를 접목해 **전역적 맥락 정보를 처리**하려는 시도가 증가
- 앞서 언급한 AdaBins ([AdaBins: Depth Estimation using Adaptive Bins | Papers With Code](https://paperswithcode.com/paper/adabins-depth-estimation-using-adaptive-bins#:~:text=We%20address%20the%20problem%20of,depth%20datasets%20across%20all%20metrics))나 STTR ([Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective With Transformers](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Revisiting_Stereo_Depth_Estimation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_ICCV_2021_paper.pdf#:~:text=dense%20pixel%20matching%20using%20position,tuning.%201.%20Introduction)) 모두 CNN 특징 추출 후 Transformer 모듈을 활용하여, **국소 특징과 전역 관계**를 모두 활용
- **그래프 신경망**(GNN)이나 **Spatial Propagation Network(SPN)** 계열도 깊이 보완에 쓰이는데, 이는 이미지의 픽셀들을 그래프로 보고 인접 혹은 비국소 이웃과의 관계를 학습하여 **깊이 값을 전파**
- NLSPN ([[PDF] Dynamic Spatial Propagation Network for Depth Completion](https://ojs.aaai.org/index.php/AAAI/article/view/20055/19814#:~:text=,DSPN%20%28))이나 DySPN 등이 이러한 아이디어를 딥러닝으로 구현한 예로, 전통적 **Joint bilateral filter**나 **Markov Random Field** 기반 깊이 보간을 **네트워크 레이어**로 대체
- 이러한 학습형 필터링은 에지 보존 등 **기존 공간 도메인 기법의 장점**을 살리면서, 매 장면마다 최적의 필터를 **데이터에 맞게 학습**할 수 있다는 이점

- **멀티스케일 피처 융합**도 거의 모든 최신 모델에 사용
- 깊이 추정이 **국소적인 디테일**(예: 물체 경계)과 **전역적인 장면 구조**(예: 원근감)를 모두 필요로 하기 때문
- 따라서 **FPN(Feature Pyramid Network)**처럼 계층별 특징을 결합하거나, 디코더에서 **업샘플 단계마다 스킵 연결**로 고해상도 특징을 주입하는 등의 설계가 일반화
- SPAD 업샘플링 네트워크는 4단계에 걸쳐 2배씩 업샘플하며 각 단계마다 encoder의 대응 특징과 concat하여 미세 정보를 복구([A 256 &times; 256 LiDAR Imaging System Based on a 200 mW SPAD-Based SoC with Microlens Array and Lightweight RGB-Guided Depth Completion Neural Network](https://www.mdpi.com/1424-8220/23/15/6927#:~:text=implementation%20and%20key%20functional%20component,4%20introduces%20the%20proposed%20RGB))
- 모노큘러나 스테레오 신경망들도 일반적으로 encoder-decoder 구조를 채택하여 이러한 멀티스케일 통합을 수행

- **자기지도(self-supervised) 학습**과 **반지도(semi-supervised)** 
  - 모노큘러 자기지도 경우, **뷰 재투영(뷰 합성)** 기반의 포토메트릭 손실을 사용해 깊이와 카메라 포즈를 함께 학습하는 접근
  - 2017년 등장한 Zhou 등 SfM-Learner 이후 발전하여, Monodepth2, PackNet 등으로 **성능과 안정성**이 개선
  - **스테레오 카메라 쌍**을 자기지도 학습에 활용하거나, **LiDAR 포인트 클라우드**로부터 투영된 희소 깊이를 약한 지도신호로 활용하는 연구
  - 레이더-카메라 깊이 학습에서 **강건한 데이터 증강**(Radar noise 모사 등)을 통해 **학습 신뢰성**을 높이는 시도가 있었으며 ([[PDF] Rethinking Supervision in Radar-Camera Depth Completion](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06528.pdf#:~:text=%5BPDF%5D%20Rethinking%20Supervision%20in%20Radar,image%20and%20radar%29)), 이벤트 카메라의 경우 **동시 촬영 프레임과의 일치**를 자기지도 신호로 쓰기도 했습니다 ([Self-supervised Event-based Monocular Depth Estimation using Cross-modal Consistency](https://arxiv.org/html/2401.07218v1#:~:text=supervised%20event,based%20methods))
  - 자기지도 방식은 **라벨 획득이 어려운** 자율주행 시나리오에서 효과적이며, 도메인 간 적응력도 상대적으로 높아 실제 주행환경 적용에 유리

- 전통적인 **공간 도메인(spatial domain) 방법**과 비교하면, 딥러닝 접근은 크게 두 가지에서 이점
  - 과거 **스테레오 매칭**의 SGM 등의 방법이나 **다중뷰 삼각측량**은 사전 정의된 에너지 최소화 문제를 풀었지만, 딥러닝은 **방대한 데이터로부터 학습된 휴리스틱**으로 더 복잡한 상황에서도 잘 작동
  - 텍스처가 없는 도로나 반복 무늬에서는 전통기법은 오차가 컸지만, 딥러닝 모델은 주변 문맥을 활용해 합리적 추정 함
  - **실시간 처리** 측면에서 딥러닝이 전용 하드웨어를 활용하면 오히려 빠를 수 있음
  - 과거 MVS와 BA기반 SFM은 정확하지만 느렸던 반면, 이제 **엔드투엔드 모델**은 병렬처리로 주행중 실시간 깊이 추정을 구현 ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=36%20SemAttNet%20code%202,IEEE%20Access%202022))
  - (단점) **대량의 주석 데이터 의존성**이나 **분포 변화에 대한 민감성(도메인 일반화 이슈)**
  - (보완) **도메인 랜덤화**나 **사전학습(fine-tuning)** 기법, 그리고 앞서 언급한 자기지도 학습이 함께 활용

## 4. 주요 연구 사례 및 성능 분석  


- **LiDAR+카메라 Depth Completion**: KITTI 공식 벤치마크 기준, 2020년 **NLSPN (ECCV 2020)**은 RMSE ~741.7mm의 성능을 기록하며 당시 SOTA ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=67%20NLSPN%20code%201,European%20Conference%20on))
- **SemAttNet (IEEE Access 2022)**은 **장면의 세그멘테이션 정보를 결합**하여 약 709.4mm RMSE까지 향상 ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=36%20SemAttNet%20code%202,IEEE%20Access%202022))
- 2023년 **RigNet (ECCV 2022)**과 **LRRU (ICCV 2023)** 등은 700mm 이내로 진입했고, 최신 **DMD^3C (연구용 비공개, 2025)** 기법은 약 **678.1mm RMSE**로 최고 성능 ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=Method%20Setting%20Code%20iRMSE%20iMAE,5%20Ghz%20%28Python))
- 아래 표는 일부 방법들의 KITTI Depth Completion 테스트 성능 비교:

   | **방법 (연도)**           | **RMSE (mm)** ↓ | **특징**                      
   |--------------------------|-----------------|------------------------------
   | NLSPN (ECCV 2020) ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=67%20NLSPN%20code%201,European%20Conference%20on))   | 741.7          | 비국소 이웃 전파로 희소깊이 보완       
   | SemAttNet (Access 2022) ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=36%20SemAttNet%20code%202,IEEE%20Access%202022)) | 709.4          | 세그멘트 정보 결합한 주의 메커니즘    
   | RigNet (ECCV 2022) ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=39%20RigNet%202,ECCV%202022))    | ~713           | 반복적인 영상-깊이 특징 안내(fusion) 
   | LRRU (ICCV 2023) ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=17%20HFFNet%20%201,ICCV%29%202023))    | ~695           | 장단기 순환 업데이트로 고속화        
   | DMD^3C (KITTI’25) ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=Method%20Setting%20Code%20iRMSE%20iMAE,5%20Ghz%20%28Python)) | **678.1**      | 멀티모달 융합 및 Transformers       

   *주*: ↓ 표시는 낮을수록 좋음을 의미. 각 방법의 수치는 KITTI Depth Completion 테스트 세트 기준. 

  - 최근 5년간 LiDAR-RGB 융합을 통한 깊이 완성 성능이 **약 8~9% 향상**(RMSE 기준)
  - 2024년 발표된 **Bilateral Propagation (BP-Net, CVPR 2024)**, **Tri-Perspective View Decomposition (CVPR 2024)** 등은 **깊이 맵의 경계 품질과 전역 일관성**을 크게 개선하여, 단순 오차 감소 뿐 아니라 **시각적으로도 매끄러운 깊이 맵**을 산출함 ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=J,CVPR%202024)) ([The KITTI Vision Benchmark Suite](https://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion#:~:text=2,CVPR%20%28oral%29%202024))
  - 곧 **객체 경계에서의 오차 감소**와 **얇은 구조물의 재현**으로 이어져, 3D 객체 인식이나 자유주행 경로 계획의 신뢰도를 높임

- **Monocular Depth Estimation**: 모노큘러 깊이 추정의 경우, **절대 상대 오차(Abs Rel)**와 **로그 RMSE** 등의 지표로 평가
  - KITTI Eigen 스플릿 기준으로 2020년대 초 **BTS (2019)**가 AbsRel ≈0.059, δ1(1.25 배수 정확도) ≈0.885 수준이었는데, **AdaBins (CVPR 2021)**은 Transformer 도입으로 AbsRel을 ≈0.052까지 낮추고 δ1 ≈0.903으로 향상
  - 2022년 **NeW CRFs**나 2023년 **DepthFormer** 등은 추가 개선을 이뤄 **AbsRel ~0.045** 수준에 도달
  - **자기지도 monocular**의 경우 절대 스케일이 맞지 않으므로 **스케일 정규화된 기준**으로 평가하는데, Monodepth2(2019) 이후로 **PackNet(2020)**, **HR-Depth(2021)** 등으로 오류가 지속 감소하여 **AbsRel ~0.10 → 0.085** 수준까지 개선
  - **기존 지도학습 절반 이하의 오차**로 접근하고 있어, 일부 응용에서는 라이다 없이 카메라 영상만으로도 충분한 품질의 뎁스 인식을 시도

- **Stereo Depth Estimation**: KITTI 2015 Stereo benchmark에서 **D1 에러율**(3픽셀 이상 오차 비율)이 주요 지표
  - 딥러닝 등장 전인 2014년 SGM 등의 에러율이 5% 이상이었는데, **PSMNet(2018)**으로 2.3%, **GANet(2019)** 2.0%, **LEAStereo(2020)** 1.9%까지 내려갔습니다. **STTR(2021)**은 도메인 일반화를 강조해 학습 없이도 4% 내외의 오류율을 보이며, **RAFT-Stereo(2021)**는 1.5% 수준까지 낮춘 기록 ([Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective With Transformers](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Revisiting_Stereo_Depth_Estimation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_ICCV_2021_paper.pdf#:~:text=dense%20pixel%20matching%20using%20position,tuning.%201.%20Introduction))
  - 최신 **CREStereo (CVPR 2022)**나 **AnyNet** 등은 **0.5% 이하의 에러율**을 목표로 하고 있으며, 이는 사실상 렌즈 기준 수 cm 이내의 오차만을 허용하는 매우 정확한 수준
  - 종합하면, 스테레오 매칭은 실시간 구현이 가능한 수준(모델 경량화 및 TRT 최적화 시 30FPS 이상)으로 발전하면서도 정밀도는 계속 향상

- **특수 센서 활용 Depth**: 이벤트 카메라는 전통적 지표 대신 **입체정합 오차**나 **재구성 정확도**로 평가
  - 2021년 **Event-Intensity Stereo** 연구는 **MVSEC** 데이터셋에서 이벤트만으로 한 경우 대비 ~30% 정확도 향상을 보고 ([Event-Intensity Stereo: Estimating Depth by the Best of Both Worlds](https://openaccess.thecvf.com/content/ICCV2021/papers/Mostafavi_Event-Intensity_Stereo_Estimating_Depth_by_the_Best_of_Both_Worlds_ICCV_2021_paper.pdf#:~:text=MVSEC%20,38)) ([Event-Intensity Stereo: Estimating Depth by the Best of Both Worlds](https://openaccess.thecvf.com/content/ICCV2021/papers/Mostafavi_Event-Intensity_Stereo_Estimating_Depth_by_the_Best_of_Both_Worlds_ICCV_2021_paper.pdf#:~:text=input%20sources.%20Our%20Event,world%20MVSEC%20%5B49))
  - **DSEC (2021)** 벤치마크 기준으로도 이벤트+프레임 융합이 최고 성능을 달성했
  - 열화상/멀티스펙트럼의 경우 아직 통일된 공개 벤치마크는 없지만, 개별 논문에서 **밤/낮 혼합 데이터셋**으로 RGB 단독 대비 오류 ~20–30% 감소를 보고하고 있습니다. **Radar+Camera Fusion**은 nuScenes 기반 실험에서 모노큘러 대비 **절대 깊이 MAE 25.6% 감소**를 달성했다고 보고 ([RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale](https://arxiv.org/html/2401.04325v2#:~:text=points%20and%20image%20patches%2C%20and,4DRadarCam%20dataset%2C%20respectively.%20Our%20code))
  - 레이더로 **절대 거리 스케일**을 보정한 효과로, 원거리 차량까지 오차를 크게 줄인 결과

- 표준 데이터셋(KITTI, nuScenes 등)과 평가지표(RMSE, AbsRel 등)로 정량 비교되며, 대부분의 최신 연구는 논문과 함께 **코드 공개**를 통해 재현성을 제공
- SuperFusion ([SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map Generation](https://arxiv.org/html/2211.15656v3#:~:text=driving,com%2Fhaomo)), RadarCam-Depth ([RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale](https://arxiv.org/html/2401.04325v2#:~:text=the,com%2FMMOCKING%2FRadarCam)), AdaBins ([AdaBins: Depth Estimation using Adaptive Bins | Papers With Code](https://paperswithcode.com/paper/adabins-depth-estimation-using-adaptive-bins#:~:text=We%20address%20the%20problem%20of,depth%20datasets%20across%20all%20metrics)) 

## 5. 기존 한계점 및 최적화 가능성  
1. **도메인 일반화 문제**
- 네트워크가 특정 데이터셋(예: 맑은 날씨 주간의 KITTI)에서 학습되면, 다른 환경(야간, 비, 안개 등)이나 다른 카메라 세팅에 적용할 때 성능이 저하되는 문제
- **라이다-카메라 캘리브레이션 오차**, **조도 차이**, **센서 노이즈 분포 차** 등
- **도메인 적응**(Target domain에서 미세 조정)이나, 아예 학습 시 다양한 도메인을 혼합한 **generalization training**이 시도되고 있지만 완전한 해결책은 아님
-  향후 **대규모 멀티도메인 학습**이나 **Domain invariant feature 학습** 등이 필요

2. **실시간 처리와 경량화** 이
- Transformer 기반 최신 모델들은 수백 MB 규모 파라미터와 많은 연산량으로 임베디드 시스템에서 돌리기 어려운 경우
- 모델 경량화를 위한 **지식증류(Knowledge Distillation)**, **네트워크 아키텍처 검색(NAS)** 등을 통해 연산량을 줄이는 최적화가 연구
- 2023년 LiRCDepth는 경량 모델에 대형 모델의 지식을 증류하여 레이더-카메라 깊이 추정의 효율을 높임 ([LiRCDepth: Lightweight Radar-Camera Depth Estimation via ...](https://www.researchgate.net/publication/387351339_LiRCDepth_Lightweight_Radar-Camera_Depth_Estimation_via_Knowledge_Distillation_and_Uncertainty_Guidance#:~:text=LiRCDepth%3A%20Lightweight%20Radar,knowledge%20distillation%20to%20enhance))
- 중요 영역에만 고해상도 처리를 하고 배경은 저해상도로 처리하는 **적응형 해상도** 기법이나, **스파스 컨볼루션**을 활용해 불필요한 계산을 배제하는 방법도 고려

3. **고정밀/고신뢰도 추정**
- **최악의 경우 오차**(worst-case error)를 낮추는 것
- **불확실성 추정(uncertainty estimation)**을 병렬로 수행하여 자신 없는 픽셀을 식별하거나, **사후 최적화**를 통해 거칠게 추정된 깊이를 한 번 더 보정하는 연구
- 깊이 맵과 함께 **confidence map**을 예측하여 downstream 모듈이 활용하도록 하고 있으며 ([Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective With Transformers](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Revisiting_Stereo_Depth_Estimation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_ICCV_2021_paper.pdf#:~:text=has%20several%20advantages%3A%20It%201,tuning.%201.%20Introduction)), 또는 **MLE-MAP 프레임워크**로 심도값의 확률분포까지 추정
- **안전 마진**을 두고 주행하거나, **다중 센서 크로스체크**를 통해 오검출을 상쇄하는 시스템적 대응이 고려

4. **어려운 사례에 대한 한계**
- 투명하거나 반사율이 높은 표면(예: 유리, 물웅덩이), 매우 멀리 있는 물체, 또는 동적 객체(예: 달리는 보행자)에 대한 정확한 깊이 추정
- LiDAR는 유리에 투과해버리거나 물체 표면 특성에 영향받고, 카메라는 모노큘러로는 투명체 뒤를 볼 수 없
- **학습 데이터에 이들 어려운 사례를 증강**하거나, **레이더**처럼 관통성이 있는 센서를 추가로 활용하는 연구로 보완
- 일부 연구는 **폴라라이제이션 카메라**로 물이나 유리 표면의 존재를 감지해 별도 처리하거나, **적응형 레이저 출력 LiDAR**로 반사 손실을 줄이려 함
- 이러한 방향으로 **센서계의 혁신과 알고리즘의 결합**이 요구

5. **인식 통합 및 3D 이해** 관점에서의 최적화 가능성
- 깊이 추정은 객체 인식, 분할, SLAM 등과 밀접히 연결되므로, 단일 모달의 깊이 정확도 향상도 중요하지만 **다중 작업을 공동 최적화**하면 상호 보완 효과가 
- **multi-task learning**으로 **깊이+표면법선+분할** 등을 함께 학습시키는 시도가 그러한 예로, 한 작업의 피드백이 다른 작업의 특성 학습을 도와줌
- **BEV(Bird’s Eye View) 표현**으로 깊이를 변환하여 3D 객체 검출이나 경로계획과 **엔드투엔드로 연결**하는 것도 연구

## 6. 결론 및 향후 전망  
- **멀티센서 융합, 딥러닝 아키텍처 혁신, 자기지도 학습**
- LiDAR와 카메라의 결합을 통해 희소한 깊이 데이터를 고밀도로 채워주는 기술이 성숙 단계
- 모노큘러 및 스테레오 깊이 추정은 Transformer 등 신기술 도입으로 정밀도와 안정성이 향상
- 이벤트 카메라, 적외선, 레이더 등 **비전 이외의 센서**들도 깊이 인식에 통합됨으로써, 다양한 환경에서 견고한 3D 인식을 달성하려는 노력
- ([Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning](https://arxiv.org/html/2503.00793v1#:~:text=Deploying%20depth%20estimation%20networks%20in,fuse%20strategy%2C%20for%20the%20depth))

향후 연구 방향
-  **한계 상황 극복**과 **경량화된 실시간 구현**, 그리고 **완전한 센서퓨전**이 제시
- 폭우나 폭설 같은 환경에서 **라이더+레이더+카메라+열상** 등 모든 센서 정보를 활용해 신뢰도 높은 깊이 지도를 얻는 연구
- 딥러닝 모델의 복잡도가 증가함에 따라, **에지 컴퓨팅** 환경에서도 돌아갈 수 있는 효율적인 모델 설계가 중요하며, **프루닝(pruning)**, **양자화(quantization)** 등의 기법 적용도 예상
- **사람처럼 3D 환경을 이해**하는 궁극적인 목표를 위해, 깊이 추정 모델을 3D 객체 인식, 동적 장면 분리와 통합한 **총합적 인식 시스템**으로 발전시키는 추세
- **정적 지도 + 동적 물체**를 동시에 다루는 **4D 뎁스 추정**이나, **연속적 장면 표현**과 접목된 형태

- 논문과 함께 공개한 코드 ([SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map Generation](https://arxiv.org/html/2211.15656v3#:~:text=driving,com%2Fhaomo)) ([RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale](https://arxiv.org/html/2401.04325v2#:~:text=the,com%2FMMOCKING%2FRadarCam))

