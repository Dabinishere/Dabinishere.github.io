---
revision: 0
title: "[스터디] Normalization의 종류와 장단점에 대하여"
category: programming
tags:
  - study
  - normalization
created_at: 2025-03-04 15:51:42 +09:00
last_modified_at: 2025-03-04 16:51:42 +09:00
excerpt: "LN, BN, GN - 0304"
---

# Normalization의 종류와 장단점에 대하여

### **Layer Normalization (LN), Batch Normalization (BN), Group Normalization (GN)의 차이점**  

| **구분**            | **Batch Normalization (BN)**        | **Layer Normalization (LN)**      | **Group Normalization (GN)**      |
|--------------------|--------------------------------|--------------------------------|--------------------------------|
| **정규화 범위**    | 배치 차원(`B`)에서 평균/분산 계산  | 개별 샘플(`C,H,W`)에서 정규화  | 그룹(`G`) 단위로 정규화         |
| **계산 방식**      | `μ_B, σ_B` (배치별 평균, 분산) | `μ_L, σ_L` (각 채널별 평균, 분산) | `μ_G, σ_G` (그룹별 평균, 분산) |
| **배치 크기에 대한 영향** | 크면 안정적, 작으면 불안정 | 배치 크기 영향 없음 | 배치 크기 영향 없음 |
| **연산 비용**      | 비교적 적음 (하지만 미니배치 필요) | 중간 (모든 샘플 별도 정규화) | 중간 (그룹 단위 정규화) |
| **적용 대상**      | CNN, RNN, Transformers | NLP, Transformers | CNN, 3D Vision |

---

### **각 정규화 방법의 동작 방식**
1. **Batch Normalization (BN)**
   - 배치 단위(`B, C, H, W`)에서 **배치별 평균과 분산**을 계산하여 정규화
   - 공식:  
     \[
     \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
     \]
   - **장점**: 학습이 빠르고, 깊은 네트워크에서도 안정적  
   - **단점**: 배치 크기가 작으면 불안정 (배치별 통계값 편차 큼), 실시간 추론에서 불편 (BN에서 사용하는 배치 통계는 추론 시 고정됨)  
   
2. **Layer Normalization (LN)**
   - 배치와 관계없이 샘플별(`C, H, W`) **각 채널 전체의 평균과 분산**을 계산하여 정규화  
   - 공식:  
     \[
     \hat{x}_i = \frac{x_i - \mu_L}{\sqrt{\sigma_L^2 + \epsilon}}
     \]
   - **장점**: 배치 크기와 무관하게 안정적, 특히 NLP/Transformers에서 잘 동작  
   - **단점**: CNN에서는 채널 간 정보 손실이 발생할 수 있음  

3. **Group Normalization (GN)**
   - `C` 개의 채널을 **G 그룹으로 나누어 각 그룹별 평균과 분산을 계산**하여 정규화  
   - 공식:  
     \[
     \hat{x}_i = \frac{x_i - \mu_G}{\sqrt{\sigma_G^2 + \epsilon}}
     \]
   - **장점**: 배치 크기에 영향받지 않으면서도 CNN에서 잘 동작  
   - **단점**: 적절한 `G` 값 선택이 중요 (작으면 LN과 유사, 크면 BN과 유사)  


---

Batch Normalization (BN), Layer Normalization (LN), Group Normalization (GN) 외에도 다양한 정규화(Normalization) 기법이 존재한다.

---

## **1️⃣ Instance Normalization (IN)**
- **개념**: 각 샘플별(`H, W`)에서 **각 채널마다 별도로 평균과 분산을 계산하여 정규화**  
- **공식**:  
  \[
  \hat{x}_{i,c} = \frac{x_{i,c} - \mu_{i,c}}{\sqrt{\sigma_{i,c}^2 + \epsilon}}
  \]
  - 여기서 `i`는 배치 차원, `c`는 채널, `μ`와 `σ`는 해당 채널의 평균/표준편차
- **특징**
  - 배치 크기에 영향을 받지 않음 (배치 크기 `1`에서도 안정적)
  - 이미지 스타일 변환(Style Transfer)에서 매우 자주 사용됨
  - **각 채널별로 독립적으로 정규화되므로, 텍스처 정보 유지에 유리**
- **사용 사례**:  
  - **스타일 트랜스퍼(Style Transfer)** 모델 (e.g., AdaIN)  
  - **이미지 생성 GAN (Generative Adversarial Networks)**  
- **사용 빈도**: ⭐⭐⭐ (스타일 변환, GAN에서 많이 사용됨)  

---

## **2️⃣ Weight Standardization (WS)**
- **개념**: **입력 데이터가 아니라, 가중치를 정규화**하는 방법  
- **공식**:  
  \[
  W' = \frac{W - \mu_W}{\sigma_W}
  \]
  - `W`는 가중치, `μ_W`, `σ_W`는 평균과 표준편차
- **특징**
  - 컨볼루션 네트워크(CNN)에서 **BN 없이도 학습 안정성 향상**  
  - 큰 모델에서 BN을 대체하는 용도로 활용  
- **사용 사례**:
  - CNN (특히 **BatchNorm을 대체**할 때)
  - ResNet, EfficientNet 등 다양한 CNN 아키텍처  
- **사용 빈도**: ⭐⭐⭐ (특정 연구 및 CNN 최적화에서 사용)  

---

## **3️⃣ Batch Renormalization (BRN)**
- **개념**: Batch Normalization의 단점을 보완한 방식  
- **공식**:  
  \[
  \hat{x} = r \cdot \left(\frac{x - \mu_B}{\sigma_B}\right) + d
  \]
  - 여기서 `r`과 `d`는 BN이 추론(inference) 시 배치 크기에 따라 변화하는 문제를 보완하는 보정값  
- **특징**
  - 작은 배치 크기에서도 BN처럼 동작 가능  
  - 하지만 BN보다 학습이 느려지며 추가적인 계산량 증가  
- **사용 사례**:  
  - BN을 적용하기 어려운 경우 (e.g., 배치 크기가 작을 때)  
- **사용 빈도**: ⭐⭐ (BN을 개선하려는 시도 중 하나지만, GN, LN이 더 많이 사용됨)  

---

## **4️⃣ Spectral Normalization (SN)**
- **개념**: **가중치 행렬의 스펙트럼 노름(최대 고유값)으로 정규화**  
- **공식**:  
  \[
  W' = \frac{W}{\sigma(W)}
  \]
  - 여기서 `σ(W)`는 가중치 행렬 `W`의 스펙트럼 노름(최대 고유값)
- **특징**
  - **GAN(Generative Adversarial Networks)**에서 많이 사용됨  
  - Generator의 훈련 안정성을 향상  
  - Convolution 연산에서도 적용 가능  
- **사용 사례**:  
  - GAN (특히 **WGAN-GP**, BigGAN)  
- **사용 빈도**: ⭐⭐⭐⭐ (GAN 모델에서 필수적)  

---

## **5️⃣ AdaIN (Adaptive Instance Normalization)**
- **개념**: Instance Normalization을 개선한 방법으로, **입력 스타일을 조절**  
- **공식**:  
  \[
  \text{AdaIN}(x, y) = \sigma(y) \cdot \frac{x - \mu(x)}{\sigma(x)} + \mu(y)
  \]
  - `x`: 원본 이미지, `y`: 스타일 이미지  
- **특징**
  - Style Transfer에서 강력한 성능 발휘  
  - **스타일 정보와 컨텐츠 정보를 효과적으로 분리**  
- **사용 사례**:  
  - 스타일 변환 (e.g., Fast Style Transfer)  
- **사용 빈도**: ⭐⭐⭐⭐ (Style Transfer에서 필수적)  

---

## **정리: 각 정규화 기법의 비교 및 사용 빈도**
| **정규화 기법**           | **정규화 방식**       | **배치 크기 영향** | **주로 사용되는 분야** | **사용 빈도** 
|--------------------|-----------------|-----------------|----------------|------------
| **Batch Norm (BN)** | 배치별 평균/분산 | 영향 큼        | CNN, 딥러닝 일반 | ⭐⭐⭐⭐⭐ 
| **Layer Norm (LN)** | 채널별 정규화   | 영향 없음      | NLP, Transformer | ⭐⭐⭐⭐ 
| **Group Norm (GN)** | 그룹 단위 정규화 | 영향 없음      | CNN, Vision     | ⭐⭐⭐ 
| **Instance Norm (IN)** | 채널별로 샘플 단위 정규화 | 영향 없음 | 스타일 트랜스퍼, GAN | ⭐⭐⭐ 
| **Weight Standardization (WS)** | 가중치 자체를 정규화 | 영향 없음 | CNN 최적화 | ⭐⭐⭐ 
| **Batch Renormalization (BRN)** | BN 개선 (보정값 추가) | 약간 영향 | BN 대체 연구 | ⭐⭐ 
| **Spectral Norm (SN)** | 가중치 행렬의 스펙트럼 정규화 | 영향 없음 | GAN (WGAN, BigGAN) | ⭐⭐⭐⭐ 
| **AdaIN (Adaptive IN)** | 스타일 조정 가능 | 영향 없음 | 스타일 변환 | ⭐⭐⭐⭐ 

---

## **결론: 언제 어떤 정규화를 사용할까?**
1️⃣ **일반적인 CNN (Image Classification, Object Detection 등)**  
   → **BatchNorm (BN) or GroupNorm (GN)** (BN이 가능하면 BN이 더 빠름)  
2️⃣ **배치 크기가 작거나 1인 경우**  
   → **LayerNorm (LN) or GroupNorm (GN)**  
3️⃣ **GAN, Style Transfer**  
   → **InstanceNorm (IN), SpectralNorm (SN), AdaIN**  
4️⃣ **딥러닝 기반 NLP (Transformer, BERT 등)**  
   → **LayerNorm (LN)**  
5️⃣ **BN 없이 CNN을 개선하고 싶다면?**  
   → **Weight Standardization (WS) + GroupNorm (GN)**  

---

💡 **요약**  
- **BN (Batch Normalization)**: 딥러닝에서 가장 일반적으로 사용됨 (하지만 배치 크기가 작으면 불안정)  
- **LN (Layer Normalization)**: NLP/Transformer에서 주로 사용됨  
- **GN (Group Normalization)**: 배치 크기 영향 없이 CNN에서 BN 대체 가능  
- **IN (Instance Normalization)**: 스타일 변환 및 GAN에서 필수적  
- **SN (Spectral Normalization)**: GAN에서 안정성 증가  

💡 **실제 프로젝트에서 배치 크기와 도메인을 고려하여 적절한 정규화 기법을 선택하는 것이 중요!** 



> ## 내 연구에서는?
> ### **배치 크기 1일 때 가장 적절한 정규화 방법**
> 1. **Batch Normalization (BN) → 비효율적!**
>   - 배치 크기가 1이면 BN의 평균/분산이 하나의 샘플 값과 동일해지므로 **정규화 효과가 사라짐**  
>   - 따라서 BN은 배치 크기가 작을 때 성능이 저하됨  
> 2. **Layer Normalization (LN) → 적절**
>   - LN은 **배치 크기와 무관**하므로, 배치 크기가 1이어도 정상적으로 동작  
>   - 특히 NLP/Transformer 모델에서 효과적  
> 3. **Group Normalization (GN) → CNN에서는 더 나은 선택**
>   - GN은 BN과 LN의 중간 형태로, 배치 크기에 영향을 받지 않음  
>   - CNN에서 BN을 대체할 경우, **GN이 LN보다 적절할 가능성이 높음**  
>   - 일반적으로 `G=32` 정도가 효과적 
>
>> **🔹 배치 크기가 1이면:**  
>> - **CNN** 모델 → **Group Normalization (GN) 추천**  
>> - **NLP/Transformer** 모델 → **Layer Normalization (LN) 추천**  
>> - **Batch Normalization (BN)는 비효율적이므로 사용 지양!**
>> - 즉, **BN은 배치 크기가 클 때 효과적**이고, **LN과 GN은 배치 크기가 1이거나 작은 경우에도 안정적으로 동작**한다.