---
revision: 0
title: "[리뷰] ADAPT: Attentive Self-Distillation and Dual-Decoder Prediction Fusion for Continual Panoptic Segmentation (ICLR 2025)"
category: programming
tags:
  - review
created_at: 2025-02-23 16:57:28 +09:00
last_modified_at: 2025-02-23 16:57:28 +09:00
excerpt: "논문 리뷰 - 0223"
---

# ADAPT: Attentive Self-Distillation and Dual-Decoder Prediction Fusion for Continual Panoptic Segmentation (ICLR 2025)


이 논문은 **연속적 범주 분할(Continual Panoptic Segmentation, CPS)** 문제를 다룬다. CPS는 **연속적으로 학습해야 하는 상황에서 의미적(Semantic) 및 객체별(Instance) 분할을 수행하는 문제**로, 기존 모델들은 **망각(catastrophic forgetting)** 문제, **확장성 부족**, **비효율적인 적응 방식** 등의 한계를 가진다.

이를 해결하기 위해 **ADAPT**라는 새로운 프레임워크를 제안한다. 
1. **주의(Self-Attentive) 기반 자기 지식 증류(Self-Distillation)**: 중요 정보 위주로 학습하여 기존 지식 손실 방지.
2. **이중 디코더 예측 융합(Dual-Decoder Prediction Fusion)**: 기존 클래스와 새로운 클래스의 정보 균형을 유지하는 결합 방식.
3. **효율적인 적응 전략**: **이미지 인코더와 픽셀 디코더를 고정**하고 특정 트랜스포머 디코더만 학습하여 연산 효율성 확보.

---

## **문제점과 해결책**
### **1. 문제점: 연속 학습에서의 망각(Catastrophic Forgetting)**
- 기존 모델들은 새로운 데이터를 학습하면서 이전에 배운 정보를 쉽게 잊어버리는 경향이 있음.
- 이는 새로운 학습 단계에서 **이전 클래스 라벨이 제공되지 않기 때문**에 발생하며, 모델이 **새로운 클래스에 편향**되는 문제를 유발.

#### **해결책**
- **주의 기반 자기 지식 증류(Self-Attentive Distillation)**:
  - 모든 픽셀과 객체를 동일하게 학습하는 대신, **이전 모델(교사 모델, Teacher Model)이 예측한 신뢰도를 기준으로 중요도를 조정**.
  - 배경(background)이나 의미 없는 정보는 가중치를 낮추고, 중요한 객체 정보는 더 강하게 유지.
- **효과**:
  - 기존 클래스의 정보를 잃지 않으면서 새로운 클래스를 효과적으로 학습.

---

### **2. 문제점: 높은 지식 증류 연산 비용(Knowledge Distillation Overhead)**
- 일반적인 **지식 증류(KD)** 방법은 **교사(Teacher) 모델과 학생(Student) 모델의 별도 순전파(forward pass)** 연산을 필요로 하며, 이는 **연산량 증가**를 초래.
- 예를 들어, **CoMFormer** 같은 기존 방법들은 전체 모델을 재학습해야 하므로 매우 비효율적.

#### **해결책**
- **공유 순전파(Shared Forward Passes) 적용**:
  - **이미지 인코더와 픽셀 디코더를 고정**하고, **교사 모델과 학생 모델이 동일한 가중치를 공유**하여 **단일 순전파로 지식 증류 가능**.
- **효과**:
  - 기존 대비 **훈련 시간 34.8% 단축** (CoMFormer 대비).
  - 불필요한 계산량 제거로 **더 적은 연산으로도 동일한 성능 유지**.

---

### **3. 문제점: 모델 유연성 부족(Plasticity)**
- 일부 방법(예: **ECLIPSE**)은 **기존 클래스 유지 성능을 높이기 위해 대부분의 가중치를 고정**하지만, **새로운 클래스 학습 성능이 저하**됨.
- 즉, 모델이 **기존 클래스를 유지하는 데 집중하면서 새로운 정보 학습이 어려워짐**.

#### **해결책**
- **선택적 트랜스포머 디코더 미세 조정(Selective Transformer Decoder Fine-Tuning)**:
  - 트랜스포머 디코더의 **자기-어텐션(Self-Attention) 레이어는 고정**하고, **교차-어텐션(Cross-Attention) 및 피드포워드 네트워크(FFN)만 미세 조정**.
- **효과**:
  - **기존 클래스 유지(Rigidity)와 새로운 클래스 학습(Plasticity)을 균형적으로 개선**.
  - **과적합을 방지하면서도 일반화 성능 향상**.

---

### **4. 문제점: 기존 및 신규 클래스 확률 불균형(Probability Scale Inconsistency)**
- 연속적 학습을 거듭할수록 **기존 및 신규 클래스 간의 확률 값 차이가 커짐**.
- 기존 **확률 기반 융합(Probability-Level Fusion, PLF)** 방식은 확률 값이 불균형하여 **기존 클래스를 과대평가**하는 경향이 있음.

#### **해결책**
- **쿼리 기반 융합(Query-Level Fusion, QLF) 적용**:
  - 기존과 신규 클래스를 **각각의 독립적인 디코더에서 예측한 후, 예측 결과를 합성**.
  - 확률 직접 융합을 하지 않고, **가장 신뢰할 수 있는 예측을 선택**.
- **효과**:
  - **확률 값 불균형 문제 해결** → 더 정확한 분할 결과 제공.
  - **연속 학습 시 오류 축적 방지**.

---

## **실험 결과**
**ADE20K 데이터셋**에서 ADAPT를 평가했으며, 기존 방법(CoMFormer, ECLIPSE) 대비 **우수한 성능을 기록**함.

- **성능 향상(Panoptic Quality, PQ 기준)**:
  - ADAPT는 **기존 CoMFormer 대비 기존 클래스에서 +6.2, 신규 클래스에서 +9.8 향상**.
  - **ECLIPSE 대비 전체 PQ 성능 3% 이상 향상**.

- **연산 효율성 개선**:
  - ADAPT는 **훈련 시간이 34.8% 단축** (기존 CoMFormer 대비).
  - **훈련 반복 횟수도 16k → 4k로 줄어듦**.
  
- **추론(Inference) 속도 개선**:
  - 연산량(FLOPs) 증가를 **6.5%로 최소화**하면서도, **신규 클래스 성능을 59.4% 향상**.

---

## **결론**
ADAPT는 **연속적 범주 분할(Continual Panoptic Segmentation, CPS)** 문제를 해결하기 위한 **효율적이고 확장 가능한 방법**이다.

1. **주의 기반 자기 지식 증류(Self-Attentive Distillation)** → 기존 지식 보존 및 새로운 정보 학습 가능.
2. **이중 디코더 예측 융합(Dual-Decoder Prediction Fusion)** → 기존 및 신규 클래스 균형 유지.
3. **효율적 적응 전략(Efficient Adaptation Strategy)** → 계산량 최소화하면서 성능 유지.

이러한 기법들은 **연속 학습에서의 망각 문제를 해결**하고, **확장성과 실용성을 개선**하며, **최신 연구 중 가장 높은 성능을 기록**한 CPS 모델을 구축하는 데 기여한다.

# review
- 지식 융합에 있어서 어떤 방식을 쓸지에 대한 관점을 얻어갈 수 있음(확률 수준 융합을 하면 scale 이 불일치 할 수 있다는 것)
- CL 이라 좀 더 어려워보일수는 있는데,  어떤 task 의 발전 방향은 self-supervised  나 CL 과 같은 추가 세부 분야 task 로 이어지기 때문에 task 에 그쪽 분야의 연구가 없다면 간단한 방법론을 가져와서 수행해보는 것도 좋지 않을까?