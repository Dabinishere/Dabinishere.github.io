---
revision: 0
title: "[리뷰] TokenBinder: Text-Video Retrieval with One-to-Many Alignment Paradigm (WACV 2025)"
category: programming
tags:
  - review
created_at: 2025-03-10 16:55:44 +09:00
last_modified_at: 2025-03-10 16:55:44 +09:00
excerpt: "논문 리뷰 - 0310"
---

# TokenBinder: Text-Video Retrieval with One-to-Many Alignment Paradigm (WACV 2025)



## **1. Introduction**
> 기존 TVR 방법들이 "일대일(one-to-one) 정렬 방식"으로 인해 유사한 비디오 후보들을 구별하는 데 어려움을 겪는 문제를 해결하기 위해, 인간의 비교 판단 방식을 모방한 "일대다(One-to-Many) 정렬 방식"을 적용한 TokenBinder 모델을 제안한다.

### **1. 연구 배경**
- **Text-Video Retrieval (TVR)** 은 텍스트를 기반으로 적절한 비디오를 검색하는 작업으로, 멀티미디어 데이터베이스에서 중요한 역할을 한다.
- 기존 TVR 방법들은 주로 **일대일(one-to-one) 정렬 방식**을 사용하여 텍스트와 비디오를 매칭한다.
- 그러나 이러한 방법들은 비슷한 후보들 간의 **미세한 차이를 구별하는 데 어려움을 겪으며**, 정확도가 낮아지는 문제가 발생한다.

### **2. 기존 연구의 한계**
- TVR의 발전 과정:
  - **초기 연구들**: 비디오와 텍스트의 정렬 방식을 개발하여 성능을 향상.
  - **CLIP 모델 등장**: CLIP을 활용한 coarse-grained (전체 특징 기반) 방법이 많이 사용됨.
  - **세밀한 정렬(fine-grained) 연구**: 비디오를 프레임 또는 객체 수준으로 나누어 보다 세밀한 정렬 방식을 제안.
  - **Coarse-to-Fine 정렬 방식**: coarse 정렬을 먼저 수행한 후 fine 정렬을 적용하는 방식이 등장.
- 하지만 대부분 **"일대일(one-to-one)" 정렬 방식**을 사용하기 때문에, **유사한 후보들 간의 차이를 구별하는 데 한계**가 존재한다.

### **3. 연구 동기**
- 인간의 인지 과정에서는 **비교 판단(Comparative Judgement)** 을 통해 대상을 평가함.
  - 즉, 개별적으로 평가하는 것이 아니라, 여러 후보를 비교하면서 상대적인 차이를 파악함.
- 이러한 개념을 차용하여 **"일대다(One-to-Many) 정렬 방식"** 을 적용하면 보다 효과적인 검색이 가능할 것이라고 가정.

### **4. 연구 목표 및 기여**
- 본 논문에서는 **"TokenBinder"** 라는 새로운 TVR 프레임워크를 제안한다.
  - **일대다(One-to-Many) Coarse-to-Fine 정렬** 방식을 적용하여 기존 방법의 한계를 극복.
  - **두 단계 검색 과정** (broad-view retrieval → focused-view retrieval) 을 통해 성능 향상.
  - **Query Indicators** 라는 새로운 개념을 도입하여, 텍스트의 중요한 특징을 보존하면서 비디오와 정밀하게 정렬.
- 이를 통해 **기존 SOTA (State-of-the-Art) 모델들보다 뛰어난 성능을 입증**하며, TVR 연구 분야에 기여.

---

## **2. Related Work**  
> 기존 TVR 방법들은 coarse 또는 fine-grained alignment 방식을 사용하지만, 대부분 "일대일 정렬" 방식의 한계를 가지고 있다. 이를 해결하기 위해 TokenBinder는 "일대다 정렬"과 "두 단계 검색 방식"을 도입하여 더 정밀한 검색을 가능하게 한다.



### **2.1. Text-Video Retrieval**
- **TVR의 주요 목표**: 텍스트 쿼리에 가장 적절한 비디오를 검색하는 것.
- **두 가지 주요 작업**:
  - **텍스트-비디오 검색 (Text-to-Video, T2V)**: 텍스트를 입력하면 가장 적절한 비디오를 찾는 것.
  - **비디오-텍스트 검색 (Video-to-Text, V2T)**: 비디오를 입력하면 적절한 설명(텍스트)을 찾는 것.
- 기존 연구에서는 주로 **T2V 검색**을 중심으로 진행됨.

---

### **2.2. Multimodal Alignment in Text-Video Retrieval**
TVR에서 가장 중요한 것은 **텍스트와 비디오 간의 정렬(Alignment) 방식**이며, 기존 연구들은 크게 세 가지로 나뉜다.

#### **(1) Coarse-Grained Alignment (거친 정렬 방식)**
- 텍스트와 비디오를 각각 하나의 글로벌 토큰으로 표현하고, 단순한 유사도 계산(코사인 유사도 등)으로 정렬하는 방식.
- 대표적인 연구:
  - **CLIP 기반 방법 (Luo et al., 2021)**: 이미지-텍스트 정렬을 학습한 CLIP 모델을 비디오-텍스트 정렬에 적용.
- **한계**: 세부적인 특징을 반영하지 못해, 비슷한 비디오를 구별하기 어렵다.

#### **(2) Fine-Grained Alignment (세밀한 정렬 방식)**
- 텍스트와 비디오를 더 작은 단위(단어, 프레임, 객체 등)로 나누어 정렬하는 방식.
- 대표적인 연구:
  - **Hierarchical Fusion Networks**: 텍스트를 단어/문장 단위로, 비디오를 프레임/객체 단위로 나누어 매칭.
- **한계**: 연산 비용이 크고, 속도가 느려질 수 있음.

#### **(3) Coarse-to-Fine Alignment (거친-세밀 정렬 방식)**
- Coarse 정렬을 먼저 수행한 후, Fine 정렬을 적용하는 **2단계 방식**.
- 대표적인 연구:
  - **Hybrid 방법론 (Tian et al., 2024)**: coarse 정렬로 후보를 좁힌 후, fine 정렬로 최종 매칭 수행.
- **한계**: 대부분의 연구가 **일대일(one-to-one) 정렬**을 기반으로 하여, 유사한 비디오 간의 차이를 구별하기 어려움.


#### **(4) TokenBinder의 차별점**
기존 연구의 한계를 해결하기 위해 TokenBinder는 다음과 같은 **새로운 접근 방식**을 제안한다.
1. **One-to-Many Alignment (일대다 정렬)**  
   - 기존의 일대일 정렬과 달리, 여러 후보 비디오를 동시에 비교하여 가장 적절한 비디오를 찾음.
   - 인간이 물체를 비교할 때 단일 판단이 아니라 여러 대상을 비교하는 방식에서 착안.

2. **Two-Stage Retrieval (두 단계 검색 방식)**
   - **Stage 1: Broad-view Retrieval (거친 검색)**: 코사인 유사도를 활용해 빠르게 후보를 선정.
   - **Stage 2: Focused-view Retrieval (정밀 검색)**: Cross-Attention을 이용하여 후보 간의 차이를 분석.

3. **Query Indicators 도입**
   - 텍스트에서 핵심 정보를 추출하여 비디오와의 정렬을 강화.
   - 기존 방식보다 더 세밀한 정보 매칭이 가능.

---
## **3. Method**  
> TokenBinder는 기존 TVR 방식의 한계를 극복하기 위해 "일대다 정렬" 방식을 도입하고, "두 단계 검색 방식"을 활용하여 텍스트-비디오 검색을 정밀하게 수행하는 모델이다.

---

### **3.1. Preliminaries**
- Text-Video Retrieval (TVR)은 **텍스트-비디오 간 정렬 문제**를 해결하는 작업.
- **T2V (Text-to-Video)**: 텍스트 쿼리에 가장 적절한 비디오를 검색.
- **V2T (Video-to-Text)**: 비디오에 가장 적절한 텍스트 설명을 검색.
- 기존 CLIP 기반 TVR 방식은 보통 텍스트와 비디오를 **"일대일 정렬"** 방식으로 매칭.
- 하지만, 유사한 후보들을 비교할 수 없다는 한계가 있음.
- **➡ TokenBinder는 이러한 한계를 해결하기 위해 "일대다(One-to-Many) 정렬" 방식을 제안.**

---

### **3.2. Overall Framework**
TokenBinder는 **"두 단계 검색 방식 (Two-Stage Retrieval)"** 을 도입하여 보다 정밀한 검색을 수행한다.

#### **(1) Stage 1: Broad-view Retrieval**
- 텍스트와 비디오를 전반적으로 비교하여 **후보 비디오를 빠르게 선정**하는 단계.
- CLIP 기반 **코사인 유사도(Cosine Similarity)** 를 사용하여 비디오 후보를 정렬.
- **Query Indicators**  
  - 텍스트에서 중요한 정보를 담은 특수한 토큰을 생성하여 검색 성능을 향상.

#### **(2) Stage 2: Focused-view Retrieval**
- Broad-view 단계에서 선택된 **Top-K 후보 비디오들**을 보다 정밀하게 비교.
- **교차 주의 메커니즘 (Cross-Attention Transformer)** 을 사용하여 각 후보와 텍스트를 정밀하게 매칭.
- **MLP (Multi-Layer Perceptron)** 을 통해 최종적인 비디오 유사도를 계산.

---

### **3.3. Feature Representation**
TokenBinder는 CLIP 모델을 활용하여 **텍스트와 비디오 특징을 효과적으로 표현**한다.

#### **(1) 텍스트 특징 (Text Feature)**
- **Query Indicators** 추가: 텍스트의 중요한 부분을 추출하여 추가적인 정보 제공.
- CLIP의 **Self-Attention** 기법을 활용하여 텍스트 내 정보 흐름을 최적화.

#### **(2) 비디오 특징 (Video Feature)**
- **비디오를 프레임 단위로 분할**하여 특징을 추출.
- CLIP의 **Vision Transformer (ViT)** 기반 모델을 활용.
- **Temporal Mean Pooling** 을 적용하여 프레임 간 특징을 압축.

---

### **3.4. Broad-view Retrieval**
- Query Indicators를 이용해 텍스트의 전반적인 특징을 생성.
- CLIP의 코사인 유사도를 기반으로 전체 데이터베이스의 비디오와 비교하여 초기 후보를 선택.
- 식:
  $$
  S'_i = (I^T_{1, i} \cdot f_{[CLS], 1}, I^T_{1, i} \cdot f_{[CLS], 2}, ..., I^T_{1, i} \cdot f_{[CLS], N})
  $$
  - 여기서 \( S'_i \)는 **텍스트와 비디오 간 유사도 점수**.

---

### **3.5. Focused-view Retrieval**
- Broad-view 단계에서 선택된 **Top-K 비디오 후보**를 세밀하게 비교.
- **교차 주의(Cross-Attention Transformer)** 를 적용하여 각 비디오와 텍스트 간 연관성을 학습.
- **Gumbel-Softmax 기법**을 사용하여 차별적인 특징을 추출.
- **MLP를 활용하여 최종 유사도 점수 계산**:
  $$
  \{∆1, ∆2, ..., ∆k\} = MLP(I_2, I_3, ..., I_m)
  $$
- **최종 유사도 계산 식**:
  $$
  S_i = (S'_i,1 + ∆1, S'_i,2 + ∆2, ..., S'_i,k + ∆k)
  $$

---

### **3.6. Broad- and Focused-view Supervision Loss**
TokenBinder는 두 단계에서 각각 다른 손실 함수(Loss)를 적용하여 학습한다.

#### **(1) Broad-view Retrieval 손실 함수**
- **Contrastive Loss (대조 손실)** 을 사용하여 텍스트와 비디오 간의 차이를 극대화.
  $$
  ℓ_{t→v} = - \frac{1}{B} \sum_{i=1}^{B} \frac{e^{I^T_{1,i} \cdot f_{[CLS], i} / \tau}}{\sum_{j=1}^{B} e^{I^T_{1,i} \cdot f_{[CLS], j} / \tau}}
  $$
  - 여기서 \( \tau \)는 온도 조절 파라미터.

#### **(2) Focused-view Retrieval 손실 함수**
- **Cross-Entropy Loss** 를 사용하여 세밀한 정렬을 학습.
  $$
  ℓ_{focus,v} + ℓ_{focus,t}
  $$

#### **(3) 최종 손실 함수**
- 두 단계 손실 함수를 결합하여 최적화.
  $$
  ℓ = (ℓ_{t→v} + ℓ_{v→t}) /2 + (ℓ_{focus,v} + ℓ_{focus,t}) /2
  $$

---
## **4. Experiments**  

> 이 섹션에서는 **TokenBinder의 성능 평가 및 실험 결과**를 상세히 분석한다.  
> TokenBinder는 6개의 벤치마크 데이터셋에서 기존 최신 모델(State-of-the-Art, SOTA)보다 뛰어난 성능을 보였으며, 다양한 실험을 통해 모델의 효과성을 검증했다.  



### **4.1 Datasets (데이터셋)**
TokenBinder는 **6개의 대표적인 텍스트-비디오 검색(TVR) 데이터셋**에서 평가되었다.

| 데이터셋 | 비디오 개수 | 특징 
|-----------|-----------|------------------------------------------------
| **MSR-VTT** | 10,000 | 가장 널리 사용되는 TVR 데이터셋. MSRVTT-1K 설정에서 실험 진행. 
| **DiDeMo** | 10,000 | 비디오 속 특정 순간(moment)을 찾는 것이 중요한 데이터셋. 
| **ActivityNet** | 20,000 | 긴 길이의 비디오가 포함되어 있으며, 복잡한 활동 인식이 필요. 
| **VATEX** | 34,991 | 다양한 동작과 활동을 포함한 멀티모달 데이터셋. 
| **MSVD** | 1,970 | 다양한 비디오와 상세한 주석(annotation)이 포함됨. 
| **LSMDC** | 101,079 | 영화에서 추출된 비디오 클립으로 구성됨. 

 

---

### **4.2 Evaluation Metric (평가 지표)**  
TokenBinder의 성능은 **다음과 같은 평가 지표**를 사용하여 측정되었다.

1. **Recall@K (R@K)**
   - 검색된 상위 K개의 결과 내에 정답이 포함될 확률.
   - **R@1, R@5, R@10**을 사용하여 측정.
   - 값이 클수록 검색 정확도가 높음.

2. **Median Rank (MdR)**
   - 정답이 처음 등장하는 **순위의 중앙값**.
   - 값이 작을수록 검색 성능이 우수함.

3. **Mean Rank (MnR)**
   - 정답이 등장하는 평균 순위.
   - 값이 작을수록 성능이 우수함.
 

---

### **4.3 Training Details (학습 세부 사항)**  
TokenBinder 모델은 **CLIP-ViP 프레임워크**를 기반으로 훈련되었다.

- **프레임워크**: PyTorch 1.8.0  
- **백본 모델**: CLIP ViT-B/32, ViT-B/16  
- **Optimizer**: AdamW  
- **Batch Size**: 128  
- **Learning Rate (학습률)**:
  - Focused-view Fusion Module: **1e-4**
  - 나머지 컴포넌트: **1e-6**
- **Weight Decay**: 0.2  
- **학습 Epoch 수**:
  - MSR-VTT: **5 Epoch**
  - DiDeMo, ActivityNet, MSVD, VATEX: **20 Epoch**
  - LSMDC: **10 Epoch**
 

---

### **4.4 Comparison Results (성능 비교 결과)**  
TokenBinder는 **기존 SOTA(Text-Video Retrieval 모델) 대비 높은 성능**을 기록했다.  

#### **MSRVTT-1K 데이터셋 성능 비교 (R@1 기준)**
| 모델 | 백본 | R@1 (%) 
|-----------------|--------|-------
| CLIP4Clip | ViT-B/32 | 44.5 
| X-CLIP | ViT-B/32 | 46.1 
| TS2-Net | ViT-B/32 | 47.0 
| UCOFIA | ViT-B/32 | 49.4 
| **TokenBinder (Ours)** | ViT-B/32 | **52.5** 

- **TokenBinder는 기존 모델보다 2~3% 높은 성능을 기록.**  
- **ViT-B/16을 사용하면 성능이 더욱 향상 (54.6% R@1).**


---

### **4.5 Ablation Study (구성 요소 분석 실험)**  
**각 구성 요소가 모델 성능에 미치는 영향을 분석**하기 위해 다양한 실험을 수행했다.

#### **(1) Query Indicators 영향 분석**
| Query Indicators 개수 | R@1 (t2v) | R@1 (v2t) 
|----------------------|----------|----------
| 2개 | 50.6 | 50.3 
| 3개 | 51.2 | 51.1 
| **4개** (최적) | **52.5** | **52.0** 
| 5개 | 52.0 | 50.4 
| 6개 | 50.2 | 49.6 

- **결과**: Query Indicators 개수가 **4개일 때 최적의 성능을 보임.**  
- **해석**: Indicators 개수가 너무 많아지면, 모델이 불필요한 정보를 학습하여 성능이 저하될 수 있음.  

---

#### **(2) Top-K 후보 개수 영향 분석**
| Top-K | R@1 (t2v) | R@5 (t2v) 
|------|----------|----------
| 5개 | 52.1 | 75.1 
| **10개 (최적)** | **52.5** | **75.3** 
| 15개 | 52.4 | 75.5 
| 20개 | 52.4 | 75.5 
| 40개 | 51.3 | 76.5 

- **결과**: Top-K = **10개일 때 가장 우수한 성능**을 보임.  
- **해석**: 후보를 너무 많이 선택하면 연산 비용이 증가하고, 오히려 성능이 저하될 수 있음.  

---

#### **(3) Focused-View Cross-Attention 블록 개수 영향 분석**
| 블록 개수 | R@1 (t2v) | R@5 (t2v) 
|----------|----------|----------
| **1개 (최적)** | **52.5** | **75.3** 
| 2개 | 52.5 | 75.4 
| 3개 | 52.3 | 75.4 

- **결과**: Cross-Attention 블록 개수가 **1개일 때 최적의 성능**을 보임.  
- **해석**: 블록 개수를 늘리면 계산 비용이 증가하지만, 성능 향상 효과는 미미함.  

---

### **4.6 Case Study (사례 연구)**
TokenBinder가 **기존 모델(CLIP-ViP)보다 더 정확하게 검색하는 사례**를 분석했다.  

#### **예제 1**
**쿼리:** "한 에티오피아 여성이 아이에게 무엇을 잘하는지 묻는다."  
✅ **TokenBinder:** 올바른 영상 검색 성공  
❌ **CLIP-ViP:** 잘못된 영상 검색  

#### **예제 2**
**쿼리:** "파란색 유니폼을 입은 팀이 흰색 팀과 배드민턴을 하고 있다."  
✅ **TokenBinder:** 정확한 영상 검색 성공  
❌ **CLIP-ViP:** 다른 경기 영상을 검색  


---




## **기존 Text-Video Retrieval (TVR) 방법의 문제점과 TokenBinder의 해결책**  
이 논문은 **Text-Video Retrieval (TVR)**에서 발생하는 문제점을 해결하기 위해 **TokenBinder**라는 새로운 접근 방식을 제안한다.  
TokenBinder는 **기존 방법들이 가진 한계(1:1 정렬 방식의 비효율성, 세밀한 차이 구분 실패, 검색 성능의 한계 등)를 극복**하여 **보다 정교한 검색을 가능하게 하는 것**이 목표다.  
아래 표와 같이, **각 문제점과 TokenBinder의 해결책을 비교**하여 설명해보자.

| **문제점** | **기존 방법의 한계** | **TokenBinder의 해결책** | **기대 효과** 
|------------|----------------|----------------------|----------------
| **1. 기존 TVR의 One-to-One Alignment 문제** | 기존 방식들은 1:1 방식으로 텍스트와 비디오를 정렬하여 미세한 차이를 구별하는 데 어려움이 있음 | **One-to-Many Coarse-to-Fine Alignment 적용** | 여러 비디오를 한 번에 비교하여 더 정확한 검색 가능 
| **2. Coarse-Grained vs. Fine-Grained Alignment의 한계** | 기존 방법들은 Coarse 또는 Fine 중 하나에 초점을 맞춰 세부 정보 손실 가능 | **Coarse-to-Fine 단계적 검색 방식 채택** | 검색 속도를 유지하면서 세밀한 차이까지 반영 가능 
| **3. Feature Representation의 비효율성** | 기존 방식은 전역(Global) 특징만 사용하여 로컬(Local) 세부 정보가 부족함 | **Focused-view Fusion Network 도입** | 텍스트-비디오 정렬을 더 정밀하게 수행 
| **4. 검색 성능의 한계** | 기존 TVR 모델은 CLIP 기반이지만 상호 비교가 부족하여 성능이 제한됨 | **Query Indicators를 활용하여 효과적인 상호 비교 수행** | TVR 성능 향상 및 검색 효율 개선 

---

## **1. 기존 TVR의 One-to-One Alignment 문제**
### **기존 방법의 문제점**
- **대부분의 TVR 모델들은 텍스트와 비디오를 1:1 방식으로 정렬**  
- 이는 개별적으로 텍스트-비디오 간 유사도를 측정한 후, 가장 높은 유사도를 가진 비디오를 선택하는 방식  
- 하지만 **비슷한 후보군 사이에서 세밀한 차이를 구별하는 것이 어렵고, 종종 부정확한 검색 결과를 초래**  

### **TokenBinder의 해결책: One-to-Many Coarse-to-Fine Alignment 적용**
- **인간의 비교 판단(Comparative Judgment) 방식에서 착안하여 여러 비디오 후보를 동시에 비교하는 방식 도입**  
- 검색 단계:
  1. **Broad-view Retrieval**: 텍스트 쿼리를 기반으로 대략적인 후보 비디오를 선정 
  2. **Focused-view Retrieval**: 여러 후보를 동시에 비교하여 최적의 비디오를 선택  
- **Coarse-to-Fine 방식으로 필터링 및 재정렬을 수행하여 더 정밀한 검색 가능**

### **기대 효과**
- **비슷한 비디오 간의 세밀한 차이 구별 가능**  
- **검색 정확도 향상 → 보다 정교한 TVR 가능**  
- **대량 데이터에서도 빠르고 정확한 검색 수행 가능**  

---

## **2. Coarse-Grained vs. Fine-Grained Alignment의 한계**
### **기존 방법의 문제점**
- 기존 방식은 **Coarse(전역 특징) 또는 Fine(세부 특징) 중 하나에만 초점을 맞추는 경향이 있음**  
- Coarse-Grained 방식은 **검색 속도는 빠르지만, 세부 정보를 반영하기 어려움**  
- Fine-Grained 방식은 **더 정확하지만, 연산량이 증가하여 검색 속도가 느려짐**  

### **TokenBinder의 해결책: Coarse-to-Fine 단계적 검색 방식 채택**
- **Stage 1 (Broad-view Retrieval)**: 전역적인 특징을 기반으로 빠른 초기 검색 수행  
- **Stage 2 (Focused-view Retrieval)**: 필터링된 후보군 내에서 세부 정보 비교하여 최종 선택  

### **기대 효과**
- **속도와 정밀도를 모두 고려한 최적의 검색 구조 구현**  
- **세부 정보를 놓치지 않으면서도 빠른 검색 가능**  

---

## **3. Feature Representation의 비효율성**
### **기존 방법의 문제점**
- 대부분의 TVR 방식은 **비디오와 텍스트의 전역(Global) 특징만을 활용하여 검색을 수행**  
- **지역(Local) 특징을 고려하지 않아 세부적인 차이를 반영하기 어려움**  
- 예를 들어, “푸른 유니폼을 입은 팀이 배드민턴을 치고 있다”는 문장에서 "푸른 유니폼"과 같은 세부 정보가 검색에 충분히 반영되지 않음  

### **TokenBinder의 해결책: Focused-view Fusion Network 도입**
- **Cross-Attention Transformer를 적용하여 로컬(Local) 특징까지 반영하는 구조 설계**  
- **Query Indicators를 활용하여 중요한 텍스트 정보를 강조하고, 이를 비디오의 로컬 특징과 정렬**  

### **기대 효과**
- **더 정밀한 텍스트-비디오 매칭 가능**  
- **기존 방법보다 높은 검색 성능 확보**  

---

## **4. 검색 성능의 한계**
### **기존 방법의 문제점**
- 기존 CLIP 기반 TVR 모델들은 **텍스트와 비디오를 개별적으로 평가하여 최종 유사도를 계산**  
- 하지만, **비디오 간의 상대적인 차이를 고려하지 않아 최적의 검색 성능을 달성하기 어려움**  

### **TokenBinder의 해결책: Query Indicators를 활용한 효과적인 상호 비교**
- **Query Indicators를 추가하여 검색 쿼리와 비디오 간의 중요한 정보 정렬**  
- **Gumbel-Softmax를 사용하여 비디오 간 상대적인 차이를 고려한 검색 수행**  

### **기대 효과**
- **기존 CLIP 기반 TVR 모델보다 높은 검색 성능 제공**  
- **비디오 간 차이를 효과적으로 비교하여 더 정밀한 검색 가능**  

---

## **최종: TokenBinder**
TokenBinder는 **기존 TVR 방식의 문제점(One-to-One Alignment의 비효율성, Coarse vs. Fine 정보 손실, Feature Representation의 부족, 검색 성능의 한계)을 해결하는 접근법**이다.  
핵심 특징은 다음과 같다.  

1. **One-to-Many Coarse-to-Fine Alignment → 보다 정확한 검색 가능**  
2. **Coarse-to-Fine 단계적 검색 → 검색 속도 유지하면서 정밀도 향상**  
3. **Focused-view Fusion Network → 세부적인 텍스트-비디오 정렬 가능**  
4. **Query Indicators를 활용한 상호 비교 → 최적의 검색 성능 제공**  



> Review
>> 1. 일대일 매칭이 아니라 일대다 매칭이 키 포인트이고 해결책이 복잡하진 않음

