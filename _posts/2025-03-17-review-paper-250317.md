---
revision: 0
title: "[리뷰] Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer from Text to Image via CLIP Inversion (WACV 2025)"
category: programming
tags:
  - review
created_at: 2025-03-10 16:55:44 +09:00
last_modified_at: 2025-03-10 16:55:44 +09:00
excerpt: "논문 리뷰 - 0310"
---

# Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer from Text to Image via CLIP Inversion(WACV 2025)

NOVIC은 CLIP 모델을 활용하여 프롬프트 없이 실시간으로 객체명을 직접 생성하는 Open Vocabulary 이미지 분류 모델로, 기존 Zero-shot 분류 방식의 한계를 극복하고 새로운 객체까지 정확히 분류할 수 있는 획기적인 방법을 제안한다.




## **1. 연구 배경 및 목표**
- 기존의 **CLIP 기반 분류 모델**은 **텍스트 기반 프롬프트(prompt)** 를 필요로 하며, **미리 정의된 후보 레이블(candidate labels)** 내에서만 분류 가능.
- 이러한 접근 방식은 **새로운 객체를 분류하는 데 제약이 많음** → 완전히 열린(Open Vocabulary) 이미지 분류를 수행하기 어려움.
- 본 논문에서는 **NOVIC (uNconstrained Open Vocabulary Image Classifier)** 를 제안:
  - CLIP 모델의 **임베딩 공간(embedding space)** 을 활용하여 **텍스트 없이 이미지에서 직접 객체명을 생성**하는 모델.
  - 기존 방식과 달리, **어떠한 사전 정의된 후보 라벨도 필요하지 않음**.
  - Zero-shot Transfer 방식으로 **이미지에서 직접 객체 이름을 생성**할 수 있음.
  - **Autoregressive Transformer 기반 Object Decoder** 를 활용하여 CLIP 임베딩을 역변환(inversion)하여 객체명을 생성.



## **2. 기존 연구의 한계**
- **CLIP 및 기존 VLMs의 문제점**
  - CLIP은 강력한 Zero-shot 분류 모델이지만, 반드시 **후보 레이블 리스트**를 필요로 함.
  - 기존 Open Vocabulary 학습 방법들은 **레이블 제한**이 있으며, 제한된 데이터셋 내에서만 학습됨.
  - 일부 연구에서는 CLIP을 활용하여 **텍스트만으로 학습**하는 접근을 시도했지만, 객체명을 직접 생성하는 방식은 부족했음.

- **기존 Open Vocabulary 학습 방식과의 차별점**
  - 기존 방식은 **후보 리스트를 제공해야 하므로, 완전히 열린(Open) 분류가 불가능**.
  - NOVIC은 **완전히 새로운 객체라도 분류 가능하도록 설계됨**.



## **3. NOVIC의 방법론**
### **(1) NOVIC의 핵심 개념**
- CLIP 모델의 **텍스트 인코더를 역변환(invert)** 하여, 이미지에서 객체명을 직접 생성하는 모델을 제안.
- 학습 과정에서 **텍스트만을 사용하여 훈련**하며, Zero-shot Transfer 방식으로 **이미지에서도 객체명을 예측**할 수 있도록 설계.

### **(2) 모델 구조**
#### **① 이미지 임베딩 (Image Encoder)**
- CLIP의 이미지 인코더를 사용하여 **이미지를 벡터 임베딩**으로 변환.

#### **② 객체 디코더 (Object Decoder)**
- Transformer 기반 **Autoregressive Decoder** 를 활용하여, CLIP 임베딩을 객체명으로 변환.
- 객체명을 **자연어 형태로 직접 생성**하며, 후보 레이블 없이 동작.

#### **③ 학습 데이터**
- 텍스트 기반 **92M 개의 객체명 데이터셋**을 사용하여 학습.
- LLM을 활용하여 **자동 생성된 캡션 및 객체명 데이터셋**을 사용.
- CLIP 임베딩에 **노이즈(noise augmentation)** 를 추가하여, 이미지와의 간극을 줄임.

### **(3) 학습 방식**
- CLIP의 **텍스트 임베딩을 활용하여** 객체명을 학습.
- 훈련 데이터는 **순수한 텍스트 데이터**로만 구성 → Zero-shot Transfer가 가능하도록 설계.
- 노이즈를 추가하여 **텍스트와 이미지 임베딩의 차이를 보정**하는 방식 적용.



## **4. 실험 및 결과**
### **(1) 평가 데이터셋**
NOVIC은 다양한 데이터셋에서 평가되었으며, 기존 방법과 비교됨.
- **ImageNet-1K**
- **Food-101**
- **Tiny ImageNet**
- **자체 구축한 Open Vocabulary 데이터셋** 3가지:
  - **World-H**: 10개국에서 수집된 272개 독창적인 이미지.
  - **Wiki-H**: Wikipedia의 1,000개 대표 이미지.
  - **Val3K**: ImageNet-1K의 3,000개 샘플.

### **(2) 실험 결과**
NOVIC은 기존 모델 대비 **더 높은 개방형(Open Vocabulary) 분류 성능**을 기록함.

#### **① 기존 모델 대비 성능 비교**
| 모델 | ImageNet-1K (Top-1) | Food-101 | CIFAR-10 | Open Vocabulary 분류 정확도 
|------|----------------|---------|----------|------------------
| CLIP (Baseline) | 75.91% | 91.55% | 92.33% | 70.33% 
| Tag2Text | 69.15% | 85.25% | 91.08% | 69.90% 
| RAM | 65.05% | 83.24% | 90.53% | 68.65% 
| **NOVIC (Ours)** | **87.49%** | **94.27%** | **93.97%** | **77.10%** 

- **NOVIC은 기존 CLIP보다 ImageNet-1K에서 11.6% 향상**.
- **Food-101에서 94.27%로 가장 높은 성능을 기록**.
- Open Vocabulary 분류에서 **기존 방법보다 7~8% 높은 정확도를 달성**.

#### **② Open Vocabulary 성능 평가**
- 기존 모델(Tag2Text, RAM)은 **일반적인 객체(Man, Tree, Grass)** 를 과도하게 예측하는 경향이 있음.
- NOVIC은 **더 세밀하고 구체적인 객체명을 생성 가능** (e.g., **red velvet cake, electron microscope, brazil nut tree** 등).

#### **③ 모델 크기 및 속도**
- NOVIC의 Object Decoder는 **12.2M 파라미터**로 경량화됨.
- NVIDIA RTX A6000에서 **단일 이미지 분류 속도: 26ms** (배치 모드: 7ms) → **실시간 처리 가능**.



## **5. Ablation Study (구성 요소 분석)**
### **(1) 텍스트 임베딩 노이즈 적용 실험**
- CLIP의 **텍스트 임베딩에 노이즈를 추가하면 성능이 향상**됨.
- 노이즈 없이 학습한 경우, Open Vocabulary 분류 성능이 **13% 감소**.

### **(2) 학습 데이터 크기 분석**
- 학습 데이터 크기를 증가시킬수록 정확도가 향상됨.
- 특정 임계점(FT2) 이상에서는 성능 향상이 둔화됨.

### **(3) CLIP 모델 변경 실험**
- CLIP의 **더 강력한 모델(ViT-L/14, DFN-5B H/14-378)을 적용하면 정확도가 추가로 향상됨**.



## **6. 결론 및 향후 연구**
### **(1) 결론**
- **NOVIC은 프롬프트 없이 직접 객체명을 생성할 수 있는 최초의 Open Vocabulary 분류 모델**.
- **텍스트만으로 학습하여 Zero-shot Transfer가 가능하며, 기존 CLIP보다 성능이 우수**.
- **기존 CLIP 기반 방법의 한계를 극복하고, 미지의 객체도 분류할 수 있는 강력한 성능을 보임**.

### **(2) 향후 연구 방향**
- 객체 분류의 **세밀한 의미적 조정(semantic granularity) 조절 기능 추가**.
- 다국어 지원 확장 (Dictionary 기반 번역을 활용하여 다양한 언어로 학습 가능).





> Review
>> 1. CLIP의 텍스트 임베딩에 노이즈를 추가하면 성능이 향상? 이미지에 비해 한정된 정보를 가지고 있기 때문일까?
>> 2. 그 외에 다른 아이디어도 생각해볼 수 있을 논문


