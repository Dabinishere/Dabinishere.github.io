---
revision: 0
title: "[리뷰] Mitigate the Gap: Improving Cross-Modal Alignment in CLIP (ICLR 2025)"
category: programming
tags:
  - review
created_at: 2025-02-16 16:55:21 +09:00
last_modified_at: 2025-02-16 16:55:21 +09:00
excerpt: "논문 리뷰 - 0216"
---

# Mitigate the Gap: Improving Cross-Modal Alignment in CLIP (ICLR 2025)

이 논문은 CLIP(Contrastive Language-Image Pretraining)의 **모달리티 격차(Modality Gap)** 문제를 다루고 있다. CLIP은 이미지와 텍스트를 동일한 임베딩 공간에 매핑하여 멀티모달 학습을 수행하지만, 연구 결과에 따르면 이미지와 텍스트가 공간 내에서 서로 격리되어 분포하는 경향이 있다. 이는 모델의 성능을 저하시키고, 크로스모달 작업(예: 이미지-텍스트 검색, 제로샷 학습)의 효과를 제한하는 원인이 된다.

논문에서는 **AlignCLIP**이라는 방법을 제안하여 모달리티 격차를 줄이고 성능을 개선하려 한다. 이 방법은 두 가지 접근 방식을 포함한다.

1. **파라미터 공유(Parameter Sharing)**: 비전 및 언어 인코더의 트랜스포머 및 프로젝션 레이어를 공유하여 두 모달리티 간의 일관성을 높임.
2. **모달리티 내 분리(Intra-Modality Separation, IMSep)**: 같은 모달리티(이미지-이미지, 텍스트-텍스트) 내에서 의미적으로 유사한 샘플은 가깝게, 다른 샘플은 더 멀어지도록 조정.

---

## **문제점 및 해결책**
### 1. **문제점: CLIP의 모달리티 격차(Modality Gap)**
   - CLIP 모델은 이미지와 텍스트를 동일한 임베딩 공간에 배치하려 하지만, 실제로는 각 모달리티가 다른 영역에 밀집됨.
   - 이로 인해 크로스모달 검색과 같은 작업에서 성능이 저하됨.
   - 원인은 모델 초기화 및 CLIP의 대조 학습(contrastive loss) 방식에 있음.

#### **해결책**
   - **AlignCLIP에서 파라미터 공유**: 트랜스포머 및 프로젝션 레이어를 공유하여 이미지 및 텍스트 임베딩을 보다 일관되게 정렬.
   - **결과**: 평균 코사인 유사도가 증가하여 모달리티 간 정렬이 개선됨.

---

### 2. **문제점: 모달리티 내 임베딩의 과도한 집중**
   - CLIP은 이미지 및 텍스트 임베딩을 서로 다른 부분에 과도하게 집중시킴.
   - 같은 모달리티 내에서도 의미적으로 다른 샘플이 너무 가까이 위치함.

#### **해결책**
   - **모달리티 내 분리(IMSep) 적용**: 같은 모달리티 내에서 의미적으로 다른 샘플을 더욱 분리하도록 학습.
   - **결과**: 이미지-이미지 및 텍스트-텍스트 임베딩의 분포가 균형을 이루어 크로스모달 검색 성능 향상.

---

### 3. **문제점: 기존 연구의 단순한 변환 방식의 한계**
   - 이전 연구에서는 단순한 이동(isomorphic translation) 방식으로 모달리티 격차를 줄이려 했지만, 이는 임베딩 공간의 구조를 왜곡할 위험이 있음.

#### **해결책**
   - **AlignCLIP은 의미 기반 조정 적용**: 단순한 거리 기반 변환이 아니라, 의미적 구조를 고려한 모달리티 정렬 방식을 채택.
   - **결과**: 임베딩 공간의 자연스러운 구조 유지 및 성능 향상.

---

## **실험 결과**
논문에서는 CLIP, SharedCLIP(파라미터 공유만 적용), AlignCLIP(파라미터 공유 + IMSep)을 비교 실험함.

- **모달리티 정렬(Cross-Modal Alignment)**
  - AlignCLIP이 CLIP 대비 코사인 유사도 증가 (0.42 → 0.64)
  - 크로스모달 검색 성능 향상 (R@1 증가)

- **제로샷 분류 성능(Zero-Shot Classification)**
  - AlignCLIP이 CIFAR-10에서 8% 성능 향상
  - ImageNet-1K에서도 조금 개선

- **분류 성능(Linear Probing)**
  - ImageNet-1K에서 1.5% 향상
  - CIFAR-100에서 4.8% 향상

- **자연 분포 변화에 대한 강건성(Robustness to Distribution Shift)**
  - ImageNet-R, ImageNet-A에서도 AlignCLIP이 가장 높은 성능 기록

---

## **결론**
AlignCLIP은 **CLIP의 모달리티 격차 문제를 해결하면서도 성능을 유지하거나 향상**시키는 효과를 보였다. 특히,
1. **파라미터 공유**를 통해 모달리티 간 정렬을 강화
2. **모달리티 내 분리(IMSep)** 기법을 통해 같은 모달리티 내에서 의미적 차이를 더 잘 반영하도록 함
3. 크로스모달 검색, 제로샷 학습, 분류 성능을 향상시킴

이 연구는 단순한 거리 조정이 아닌 **의미 기반 구조 조정이 모달리티 격차를 효과적으로 줄이는 방법**이라는 점을 강조한다.